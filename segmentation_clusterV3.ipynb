{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f102c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytohn script for Ford Credit/Audience Segmentation Cluster Analysis\n",
    "# Alejandro Pineda, Data Scientist, VMLYR\n",
    "# 3/08/2023\n",
    "\n",
    "# Load Libraries (install as necessary)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist \n",
    "#from sklearn import StandardScalar\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "# seed # (might be possible to set numpy seed globally, but not advised? Investigate further)\n",
    "rando_n = 1234\n",
    "from lightgbm import LGBMClassifier\n",
    "import shap\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from plotly import graph_objects as go\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4184e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(file_name, col_drops):\n",
    "    \"\"\"\n",
    "    Loads data//drops columns where necessary//provides shape and dtype info\n",
    "    \n",
    "    \"\"\"\n",
    "    original_dat = pd.read_csv(file_name, low_memory=False)\n",
    "    og_shape = original_dat.shape\n",
    "    \n",
    "    dat_dropped = original_dat.drop(columns=col_drops, errors = 'ignore') # see if we can do try/except here\n",
    "    drop_shape = dat_dropped.shape\n",
    "    \n",
    "    dat_types = pd.DataFrame(dat_dropped.dtypes)\n",
    "    print(\"Header for new data:\")\n",
    "    print(dat_dropped.head())\n",
    "    print(f\"Shape of original data: {og_shape}\" )\n",
    "    print(f\"Shape of new data: {drop_shape}\")\n",
    "\n",
    "    \n",
    "    return dat_dropped, drop_shape, dat_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of our column names have weird names or tags \n",
    "# (legacy data will use tags to tell use what database it came from)\n",
    "# use reg-ex to pattern match and clean\n",
    "\n",
    "def pattern_find(columns, pattern=str):\n",
    "    \"\"\"\n",
    "    Looks for columns that match a specific pattern\n",
    "    \"\"\"\n",
    "    reg_pattern = \".*\" + pattern\n",
    "    display(f\"Looking for the following pattern: {reg_pattern}\")\n",
    "    \n",
    "    r = re.compile(reg_pattern)\n",
    "    col_list = list(filter(r.match, columns)) \n",
    "    display(col_list[0:5])\n",
    "    return col_list\n",
    "    \n",
    "pattern = \"\"\n",
    "cols_list = list(dat.columns)\n",
    "\n",
    "drop_cols = pattern_find(columns = cols_list, pattern = cluster_pat )\n",
    "\n",
    "dat.drop(cluster_cols, axis=1, inplace=True)\n",
    "dat.drop(release_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5779f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing data and identifying categorical columns by index\n",
    "\n",
    "# important function, but it's long and tedious. \n",
    "# break this down into distinct tasks (3/8)\n",
    "\n",
    "def prep_dat(data_frame, thresh=None, mean_fill = False, dat_fill=None, limit=None, na_drop=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Fills in and/or drops missing values. \n",
    "    \n",
    "    dat_fill (str): ‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None (default)\n",
    "    limit (int): the maximum number of consecutive NaN allowed\n",
    "    na_drop (str): Do you want rows or columns dropped? 'columns','index', or None (default) \n",
    "    thresh (int): Do you want to just require a specific # of non-NA values?\n",
    "    mean_fill: Do you want missing values in numerical columns filled in with column mean? Boolean\n",
    "    \"\"\"\n",
    "    \n",
    "    data_frame2 = data_frame\n",
    "    \n",
    "    num_cols = data_frame2.select_dtypes(exclude=['object']).columns\n",
    "    data_frame2[num_cols] = data_frame2[num_cols].astype(np.float32)\n",
    "    \n",
    "    # handle numerical missing nulls with the mean\n",
    "    if mean_fill:\n",
    "        num_features = list(num_cols)\n",
    "        data_frame2[num_features]  = data_frame2[num_features].fillna(data_frame2.mean())\n",
    "    \n",
    "    # handle categorical missing nulls with the specified method\n",
    "    if dat_fill:\n",
    "        data_frame2 = data_frame2.fillna(method=dat_fill, limit = limit)\n",
    "    \n",
    "    if na_drop:  \n",
    "        data_frame2 = data_frame2.dropna(axis=na_drop, thresh=thresh)\n",
    "    \n",
    "    display(f\"Data loaded for this model:\")\n",
    "    display(data_frame.head())\n",
    "    \n",
    "    display(\"Data after pre-processing: \")\n",
    "    num_features = list(data_frame2.select_dtypes(exclude=['object']).columns)\n",
    "    data_frame2[num_features] = StandardScaler().fit_transform(data_frame2[num_features])\n",
    "    \n",
    "    \n",
    "    # Identifying categorical variables based on index\n",
    "    all_cols = list(data_frame2.columns)\n",
    "    \n",
    "    cat_col_names = list(set(all_cols) - set(num_features))\n",
    "    cat_index = list()\n",
    "    for col in cat_col_names:\n",
    "        # find the index no\n",
    "        index_loc = data_frame2.columns.get_loc(col)\n",
    "        cat_index.append(index_loc)\n",
    "\n",
    "    display(f\"n_categorical features: {len(cat_index)}\")\n",
    "    display(f\"n_numerical features: {len(num_features)}\")\n",
    "    \n",
    "    display(f\"Shape of the original data set: {data_frame.shape} // Shape of new data: {data_frame2.shape}\")\n",
    "    display(\"Pre-processed data:\")\n",
    "    display(data_frame2.head())\n",
    "    return data_frame2\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f571e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns we know we won't need\n",
    "\n",
    "lil_trash = []\n",
    "\n",
    "\n",
    "file_name = ''\n",
    "trash_cols = pd.read_csv(file_name, dtype = 'str')\n",
    "trash_cols\n",
    "\n",
    "\n",
    "\n",
    "col_names = trash_cols.columns\n",
    "trash = list()\n",
    "\n",
    "for col in col_names:\n",
    "    col = trash_cols[f'{col}'].dropna()\n",
    "    trash.append(col)\n",
    "\n",
    "\n",
    "trash = [item for sublist in trash for item in sublist]\n",
    "full_trash = trash + lil_trash\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e847f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ''\n",
    "\n",
    "dat = pd.DataFrame()\n",
    "dat_shape = tuple()\n",
    "dtype = pd.DataFrame()\n",
    "\n",
    "dat, dat_shape, dtype = data_loader(file_name = filename, col_drops=full_trash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d4198",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0762ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Checking for null values (data quality is important!)\n",
    "\n",
    "nulls = dat.isnull().agg('sum')\n",
    "# nulls.to_csv('', index = False)\n",
    "nulls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc793fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat.groupby('RACE').count()\n",
    "var_count = ' '\n",
    "dat.groupby(var_count).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d7d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find integer variables\n",
    "# wrote this as a one off, should be simpler. can be re-purposed for data manipulations based on dtype.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def RepresentsInt(var):\n",
    "    if dat[var].dtype == 'object':\n",
    "        return False\n",
    "    try: \n",
    "        if (dat[var].fillna(-9999) % 1 == 0).all(): # check the modulo operator to see if there's a decimal\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except ValueError:\n",
    "        return False\n",
    "    except TypeError:\n",
    "        print(dat[var].dtype)\n",
    "        \n",
    "# map integers to int64 (size consideration?)\n",
    "counter = 0\n",
    "\n",
    "for var in dat.columns:\n",
    "    if RepresentsInt(var) == True:\n",
    "        print(var)\n",
    "        dat[var] = dat[var].astype('int64')\n",
    "        counter += 1\n",
    "\n",
    "print(counter)\n",
    "\n",
    "\"\"\"\n",
    "dat['']  = dat[''].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = dat.dtypes.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique groups in categorical var\n",
    "\n",
    "dat.groupby().nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e248a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Adding a binary var for DMA codes from the Southeast 6/1\n",
    "### commented out on 3/10\n",
    "\n",
    "\"\"\"\n",
    "dat.dropna(subset=['N2DMA'])\n",
    "display(\"Current shape of df: \" + str(dat.shape))\n",
    "\n",
    "#dat['N2DMA'] = dat['N2DMA'].astype('int64')\n",
    "se_codes_list = [525, 524, 520, 630, 575, 522, 606, 691, 557, 503, 698, 507]\n",
    "\n",
    "#dat['SE_DMA'] = dat['N2DMA'].isin(se_codes_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "se_dat = dat[dat['N2DMA'].isin(se_codes_list)]\n",
    "\n",
    "non_se_dat = dat[dat['N2DMA'].isin(se_codes_list) == False]\n",
    "\n",
    "display('Shape of se_dat: ' + str(se_dat.shape))\n",
    "display('Shape of non_se_dat' + str(non_se_dat.shape))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2874fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit = non_se_dat.shape[0] * 0.7\n",
    "\n",
    "nonse_prepped = prep_dat(non_se_dat, thresh=None, mean_fill = True, dat_fill='ffill', limit=25, na_drop='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42903dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_prepped = prep_dat(se_dat, thresh=None, mean_fill = True, dat_fill='ffill', limit=25, na_drop='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kproto_trainer(data_frame, k, random_state, num_init=3):\n",
    "    \n",
    "    proto_dat = data_frame\n",
    "    \n",
    "    num_features = list(proto_dat.select_dtypes(exclude=['object']).columns)\n",
    "    all_cols = list(proto_dat.columns)\n",
    "    \n",
    "    cat_col_names = list(set(all_cols) - set(num_features))\n",
    "    cat_index = list()\n",
    "    for col in cat_col_names:\n",
    "        # find the index no\n",
    "        index_loc = proto_dat.columns.get_loc(col)\n",
    "        cat_index.append(index_loc)\n",
    "    \n",
    "    \n",
    "    #### Initialize and train model on give df ####    \n",
    "    \n",
    "    # Leaving init and verbose as settings we don't need to worry about for now\n",
    "    kproto = KPrototypes(n_clusters=k, init='Huang', verbose=2, random_state=random_state, n_init = num_init)\n",
    "    display(f\"Training k-prototype model with {k} clusters and {num_init} centroid inits.\")\n",
    "    \n",
    "    \n",
    "    clusters = kproto.fit_predict(proto_dat, categorical=cat_index)\n",
    "    proto_dat['label'] = kproto.predict(proto_dat, categorical = cat_index)\n",
    "    data_frame_lab = proto_dat\n",
    "    \n",
    "    return kproto, clusters, data_frame_lab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow plot with cost (will take a LONG time)\n",
    "# Ran on 6/21 - running with 3 clusters\n",
    "\n",
    "\n",
    "elbow_samp = nonse_prepped.sample(n=5000, replace=False, random_state=rando_n)\n",
    "\n",
    "\n",
    "proto_dat = elbow_samp\n",
    "    \n",
    "num_features = list(proto_dat.select_dtypes(exclude=['object']).columns)\n",
    "all_cols = list(proto_dat.columns)\n",
    "    \n",
    "cat_col_names = list(set(all_cols) - set(num_features))\n",
    "cat_index = list()\n",
    "\n",
    "for col in cat_col_names:\n",
    "    # find the index no\n",
    "    index_loc = proto_dat.columns.get_loc(col)\n",
    "    cat_index.append(index_loc)\n",
    "    \n",
    "costs = []\n",
    "n_clusters = []\n",
    "clusters_assigned = []\n",
    "\n",
    "for i in tqdm(range(2, 9)):\n",
    "    try:\n",
    "        kproto = KPrototypes(n_clusters= i, init='Cao', verbose=2)\n",
    "        clusters = kproto.fit_predict(proto_dat, categorical = cat_index)\n",
    "        costs.append(kproto.cost_)\n",
    "        n_clusters.append(i)\n",
    "        clusters_assigned.append(clusters)\n",
    "    except:\n",
    "        print(f\"Can't cluster with {i} clusters\")\n",
    "        \n",
    "fig = go.Figure(data=go.Scatter(x=n_clusters, y=costs ))\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nonse_prepped = nonse_prepped.sample(n=11000, replace=False, random_state=rando_n)\n",
    "nonse_prepped.head()\n",
    "nonse_prepped.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2fc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_prepped = se_prepped.sample(n=1100, replace=False, random_state=rando_n)\n",
    "se_prepped.head()\n",
    "se_prepped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f52c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "kproto_model, kproto_cluster, lab_df = kproto_trainer(data_frame=nonse_prepped, random_state=rando_n, k=3, num_init=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce69d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "kproto_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3571ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_file = \"kproto_cluster_6_22.pkl\"  \n",
    "\n",
    "#pickle.dump(kproto_model, open(model_file, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb64750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(model_file, 'rb') as file:  \n",
    "    pickpled_kproto = pickle.load(file)\n",
    "\n",
    "pickpled_kproto\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b62f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cluster centroids of the trained model.\n",
    "print(pickpled_kproto.cluster_centroids_)\n",
    "\n",
    "# Print training statistics\n",
    "print(kproto_model.cost_)\n",
    "print(kproto_model.n_iter_)\n",
    "\n",
    "\n",
    "#kproto_model, kproto_cluster, lab_df\n",
    "lab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3ad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#color_dict = { 0:'red', 1:'blue', 2:'black', 3:'green', 4:'purple' }\n",
    "\n",
    "#c = [color_dict[i] for i in lab_df['label'] ] \n",
    "#c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebfe203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prince import FAMD\n",
    "\n",
    "\n",
    "\n",
    "famd = FAMD(n_components=2, n_iter=3, check_input=True, engine='auto', random_state=rando_n)\n",
    "\n",
    "famd = famd.fit(lab_df.drop('label', axis='columns'))\n",
    "famd.fit(lab_df)\n",
    "famd.transform(lab_df)\n",
    "\n",
    "clx = famd.plot_row_coordinates(lab_df, figsize=(15,10), color_labels = lab_df['label'])\n",
    "\n",
    "clx.get_figure().savefig('clusters_622.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9bf0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Series(kproto_cluster).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d2fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing F1 score to see how well groupings based on categories predicts data\n",
    "# https://en.wikipedia.org/wiki/LightGBM\n",
    "\n",
    "\n",
    "proto_labs = list(lab_df['label'])\n",
    "\n",
    "#Setting the objects to category \n",
    "cat_data = lab_df.drop('label', axis='columns')\n",
    "\n",
    "for i in cat_data.select_dtypes(include='object'):\n",
    "    cat_data[i] = cat_data[i].astype('category')\n",
    "\n",
    "clf_kp = LGBMClassifier()\n",
    "\n",
    "#cv_scores_kp = cross_val_score(clf_kp, cat_data, lab_df, scoring='f1_weighted')\n",
    "#print(f'CV F1 score for K-Prototypes clusters is {np.mean(cv_scores_kp)}')\n",
    "\n",
    "# So this method is taking a weak learner, a gradient boosted model, to see how well the data maps onto\n",
    "# the cluster labels using the categories\n",
    "#(https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4b03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://shap-lrjball.readthedocs.io/en/docs_update/generated/shap.TreeExplainer.html\n",
    "clf_kp.fit(cat_data, proto_labs)\n",
    "explainer_kp = shap.TreeExplainer(clf_kp)\n",
    "shap_values_kp = explainer_kp.shap_values(cat_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_kp, cat_data, color=plt.get_cmap(\"tab10\"), show=False)\n",
    "fig = plt.gcf()\n",
    "fig.set_figheight(12)\n",
    "fig.set_figwidth(14)\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('shaps_622.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d422db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Repeat analysis for SE Subset ###########################################\n",
    "\n",
    "se_kproto_model, se_kproto_cluster, se_lab_df = kproto_trainer(data_frame=se_prepped, random_state=rando_n, k=3, num_init=3)\n",
    "shap.summary_plot(shap_values_kp, cat_data, color=plt.get_cmap(\"tab10\"), show=False)\n",
    "fig = plt.gcf()\n",
    "fig.set_figheight(12)\n",
    "fig.set_figwidth(14)\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('shaps_622.png')\n",
    "# Print cluster centroids of the trained model.\n",
    "print(se_kproto_model.cluster_centroids_)\n",
    "\n",
    "# Print training statistics\n",
    "print(se_kproto_model.cost_)\n",
    "print(se_kproto_model.n_iter_)\n",
    "\n",
    "\n",
    "#kproto_model, kproto_cluster, lab_df\n",
    "se_lab_df\n",
    "\n",
    "###########################################\n",
    "\n",
    "from prince import FAMD\n",
    "\n",
    "se_famd = FAMD(n_components=2, n_iter=3, check_input=True, engine='auto', random_state=rando_n)\n",
    "\n",
    "se_famd = se_famd.fit(se_lab_df.drop('label', axis='columns'))\n",
    "se_famd.fit(se_lab_df)\n",
    "se_famd.transform(se_lab_df)\n",
    "\n",
    "clx = se_famd.plot_row_coordinates(se_lab_df, figsize=(15,10), color_labels = se_lab_df['label'])\n",
    "\n",
    "clx.get_figure().savefig('se_clusters_622.png')\n",
    "\n",
    "print(pd.Series(se_kproto_cluster).value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb346c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "\n",
    "model_file = \"SE_kproto_cluster_6_22.pkl\"  \n",
    "\n",
    "pickle.dump(se_kproto_model, open(model_file, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e21dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "\n",
    "se_proto_labs = list(se_lab_df['label'])\n",
    "\n",
    "#Setting the objects to category \n",
    "se_cat_data = se_lab_df.drop('label', axis='columns')\n",
    "\n",
    "\n",
    "for i in se_cat_data.select_dtypes(include='object'):\n",
    "    se_cat_data[i] = se_cat_data[i].astype('category')\n",
    "\n",
    "se_clf_kp = LGBMClassifier()\n",
    "\n",
    "# https://shap-lrjball.readthedocs.io/en/docs_update/generated/shap.TreeExplainer.html\n",
    "se_clf_kp.fit(se_cat_data, se_proto_labs)\n",
    "se_explainer_kp = shap.TreeExplainer(se_clf_kp)\n",
    "se_shap_values_kp = se_explainer_kp.shap_values(se_cat_data)\n",
    "#se_shap_values_kp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c023dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(se_shap_values_kp, se_cat_data, color=plt.get_cmap(\"tab10\"), show=False)\n",
    "fig = plt.gcf()\n",
    "fig.set_figheight(12)\n",
    "fig.set_figwidth(14)\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('se_shaps_622.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bc3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
